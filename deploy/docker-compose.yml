services:
  triton:
    image: hieupth/tritonserver:24.08
    container_name: rtvserving
    environment:
      HF_CONFIG_FILE: /hf.json
      HF_MODEL_REPO: /models
    volumes:
      - ./configs/hf.json:/hf.json
      - ./models:/models
      - ./hf.py:/hf.py

    tty: true
    ports:
      - "6000:8000"
      - "6001:8001"
      - "8002:8002"
    healthcheck:
      test: ["CMD", "bash", "-c", "echo -n '' > /dev/tcp/127.0.0.1/8000"]
      interval: 30s
      timeout: 2m
      retries: 2
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: ['0']
            capabilities: [gpu]
    command: >
      bash -c """python3 -u /hf.py && tritonserver --model-repository=/models --model-config-name=tensorrt"""
  
  # client:
  #   image: heronq02/pyclient:latest
  #   container_name: pyclient
    # environment:
      # QUERY_MODEL_NAME: mbert.query
      # QUERY_MODEL_VERSION: 2
      # QUERY_BATCH_SIZE: 0
      # CTX_MODEL_NAME: mbert.context
      # CTX_MODEL_VERSION: 2
      # CTX_BATCH_SIZE: 0
      # TRITON_URL: localhost:6000
      # PROTOCOL: HTTP  # or GRPC
      # VERBOSE: "true"
      # ASYNC_SET: "false"
      # QDRANT_DB: <qdrant_url>
      # QDRANT_COLLECTION_NAME: retrieval
      # TOP_K: 5
      # THRESHOLD: 0.5
      # USE_RERANK: "false"
    
    # network_mode: host
    # tty: true
    # volumes:
    #   - ./configs/deploy.yaml:/deploy.yaml
    #   - ./models:/models
    #   - ./api:/api

    # depends_on:
    #   triton:
    #     condition: service_healthy
    # command: >
    #   bash -c "serve run /deploy.yaml"