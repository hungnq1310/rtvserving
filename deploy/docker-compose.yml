services:
  triton:
    image: hieupth/tritonserver:24.08
    container_name: rtvserving
    environment:
      HF_CONFIG_FILE: /hf.json
      HF_MODEL_REPO: /models
    volumes:
      - ./configs/hf.json:/hf.json
      - ./models:/models
      - ./hf.py:/hf.py

    tty: true
    ports:
      - "6000:8000"
      - "6001:8001"
      - "8002:8002"
    healthcheck:
      test: ["CMD", "bash", "-c", "echo -n '' > /dev/tcp/127.0.0.1/8000"]
      interval: 30s
      timeout: 1m
      retries: 2
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            device_ids: ['0']
            capabilities: [gpu]
    command: >
      bash -c """python3 -u /hf.py && tritonserver \
        --model-repository=/models \
        --model-config-name=tensorrt
      """
  
  client:
    image: heronq02/pyclient:latest
    container_name: pyclient
    environment:
      QUERY_MODEL_NAME: mbert.query
      QUERY_MODEL_VERSION: 2
      BATCH_SIZE: 0
      CTX_MODEL_NAME: mbert.context
      CTX_MODEL_VERSION: 2
      BATCH_SIZE: 0
      TRITON_URL: localhost:6000
      PROTOCOL: HTTP  # or GRPC
      VERBOSE: "true"
      ASYNC_SET: "false"
      QDRANT_DB: <qdrant_url>
      QDRANT_COLLECTION_NAME: retrieval
      TOP_K: 5
      THRESHOLD: 0.5
      USE_RERANK: "false"
    
    network_mode: host
    tty: true
    volumes:
      - ./configs/deploy.yaml:/deploy.yaml
      - ./models:/models
      - ./api:/api

    depends_on:
      triton:
        condition: service_healthy
    command: >
      bash -c "serve run /deploy.yaml"